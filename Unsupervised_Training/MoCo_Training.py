# -*- coding: utf-8 -*-
"""Copy of MoCo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15uxqzhQbv8t9vnG1SSYyrT8nn21bQDUv

## Preamble
"""

import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as L

import glob as glob
import matplotlib.pyplot as plt
import numpy as np

import tensorflow_addons as tfa 
from functools import partial

import pandas as pd
import numpy as np

import json
import pickle as pk 

import os 

"""## Data Pipeline

### Read and Generate Patches
"""

# Read Image Data
@tf.function
def read_img(fpath):
  img = tf.io.read_file(fpath)
  img = tf.image.decode_jpeg(img,3)
  return img 


@tf.function
def min_upsample(img,min_size=128,dtype=tf.float32):
  '''
  Resize Image so that
  height and width will both be greater than min_size
  '''
  h = tf.shape(img)[0]
  w = tf.shape(img)[1]
  img = tf.cast(img,dtype)
  if h > min_size and w > min_size:
    return img 
  min_dim = tf.minimum(h,w)
  max_dim = tf.maximum(h,w)
  max_size = max_dim*min_size/min_dim
  max_size = tf.math.ceil(max_size)
  img = tf.image.resize(img,[max_size,max_size],preserve_aspect_ratio=True)
  return img 

@tf.function
def load_and_upscale(fpath,size=256):
  img = read_img(fpath)
  img = min_upsample(img,size)
  return img 

# Create Two Patches Given an Image
@tf.function
def gen_patches(img,patch_size=128):
  img = tf.cast(img,tf.float32)
  patch_one = tf.image.random_crop(img,[patch_size,patch_size,3])
  patch_two = tf.image.random_crop(img,[patch_size,patch_size,3])
  return (patch_one,patch_two)

"""### Data Augmentation"""

@tf.function
def random_apply(func,x,p):
  p = tf.cast(p,tf.float32)
  x = tf.cond(
      tf.less(tf.random.uniform([],0,1),p),
      lambda: func(x),
      lambda: x
  )
  return x

@tf.function
def greyscale(img):
  img = tf.image.rgb_to_grayscale(img)
  img = tf.tile(img,[1,1,3])
  return img

saturation = partial(tf.image.random_saturation,lower=0.6,upper=1.4)
contrast = partial(tf.image.random_contrast,lower=0.6,upper=1.4)
hue = partial(tf.image.random_hue,max_delta=0.1)
brightness = partial(tf.image.random_brightness,max_delta=0.4)

@tf.function
def color_jitter(img):
  img = saturation(img)
  img = contrast(img)
  img = hue(img)
  img = brightness(img)
  return img 

@tf.function
def random_zoom(imgs,lower=0.25,upper=0.75):
  #b = tf.shape(imgs)[0]
  h = tf.shape(imgs)[0]
  w = tf.shape(imgs)[1]
  scale = tf.random.uniform([],lower,upper)
  new_w = tf.math.floor(tf.cast(h,tf.float32)*scale)
  new_w = tf.cast(new_w,tf.int32)
  center_crop = tf.image.random_crop(imgs,[new_w,new_w,3])
  zoom = tf.image.resize(imgs,[h,w])
  return zoom


@tf.function
def data_aug_pipeline_two(x,y):
  '''
  Apply data augmentation to two patches from an image
  '''
  x = random_apply(color_jitter,x,0.8)
  x = tf.image.random_flip_left_right(x)
  x = random_apply(greyscale,x,0.2)
  x = random_apply(random_zoom,x,0.3)
  x = random_apply(tfa.image.gaussian_filter2d,x,0.5)
  x /= (255/2)
  x -= 1

  y = random_apply(color_jitter,y,0.8)
  y = tf.image.random_flip_left_right(y)
  y = random_apply(greyscale,y,0.2)
  y = random_apply(random_zoom,y,0.3)
  y = random_apply(tfa.image.gaussian_filter2d,y,0.5)
  y /= (255/2)
  y -= 1
  return x,y

"""## Loss Function"""

@tf.function
def NT_Xent_Loss(zis,zjs,queue,temp=0.07):
  '''
  Normalized Tempature Scaled Cross Entropy Loss
  '''

  # Normalize representations 
  # from query and key encoders
  zis = tf.math.l2_normalize(zis,-1)
  zjs = tf.math.l2_normalize(zjs,-1)

  # Get batch and queue size
  batch_size = tf.shape(zis)[0]
  queue_size = tf.shape(queue)[0]
  # Get number of negative 
  # samples from queue to retain
  take_size = queue_size - batch_size
  # Generate Labels 0 .. batch_size - 1
  labels = tf.range(batch_size)
  # Calculate similarity between 
  # query and key representations
  # Shape [Batch_size, Batch_size]
  logits_ij = tf.matmul(zis,zjs,transpose_b=True) / temp
  # similarity between query and queue
  # Shape [Batch_size, Queue_size]
  logits_iq = tf.matmul(zis,queue,transpose_b=True) / temp
  # Concat similarities [Batch_size,Batch_size + Queue_size]
  logits = tf.concat([logits_ij,logits_iq],axis=1)
  # Calculate Cross entropy loss
  loss = keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)
  loss = tf.reduce_mean(loss)
  # Update the queue
  queue = tf.gather(queue,tf.range(take_size),axis=0)
  queue = tf.concat([zjs,queue],0)
  # return loss and updated queue
  return loss, queue

def update_key_encoder(query_encoder,key_encoder,momentum=0.999):
  '''
  Momentum update for key encoder.
  '''
  q_weights = list(map(lambda x: x*(1-momentum),query_encoder.get_weights()))
  k_weights = list(map(lambda x: x*momentum,key_encoder.get_weights()))

  new_weights = [q+k for q,k in zip(q_weights,k_weights)]

  key_encoder.set_weights(new_weights)
  return key_encoder

@tf.function
def train_step(encoder_q,encoder_k,xi,xj,queue,temp=0.07):
  zj = encoder_k(xj,training=True)
  with tf.GradientTape() as tape:
    zi = encoder_q(xi,training=True)
    loss, queue = NT_Xent_Loss(zi,zj,queue,temp)
    loss += tf.reduce_sum(encoder_q.losses)

  grads = tape.gradient(loss,encoder_q.trainable_variables)

  return loss, grads, queue

def train_epoch(ds,Optim,Query,Key,Queue,steps_per_epoch):
  pbar = keras.utils.Progbar(steps_per_epoch)
  for bdx,batch in enumerate(ds,start=1):
    xi, xj = batch
    loss, grads, Queue = train_step(Query,Key,xi,xj,Queue)
    Optim.apply_gradients(zip(grads,Query.trainable_variables))
    Key = update_key_encoder(Query,Key)
    pbar.update(bdx,values=[('Loss',loss)])
    if bdx == steps_per_epoch: break
  return Query, Key, Queue

"""## Save and Load"""

def save_state(query_encoder,key_encoder,queue,optimizer,save_dir='./',step=''):
  with open(save_dir+'queue.tensor','wb') as f:
    pk.dump(queue,f)
  

  with open(save_dir+'optim.keras','wb') as f:
    pk.dump(optimizer,f)

  keras.models.save_model(query_encoder,save_dir+'query_encoder'+step)
  keras.models.save_model(key_encoder,save_dir+'key_encoder'+step)

def load_state(save_dir):
  with open(save_dir+'queue.tensor','rb') as f:
    queue = pk.load(f)
  
  with open(save_dir+'optim.keras','rb') as f:
    optimizer = pk.load(f)

  query_encoder = keras.models.load_model(save_dir+'query_encoder')
  key_encoder = keras.models.load_model(save_dir+'key_encoder')  
  return query_encoder, key_encoder, queue, optimizer

"""## Initialize Evironment"""

MODEL_PATH = '/home/ross/DenseNet201/'

# Make sure path to save model exists
if not os.path.isdir(MODEL_PATH): exit()

DATASET_PATH = '/home/ross/Datasets/'
METADATA_PATH = '/home/ross/Metadata/'

wikiart = pd.read_csv(METADATA_PATH + 'wikiart_labels.csv')
wikiart = wikiart[wikiart.train == 1]
wikiart = wikiart.relative_path.to_list()
wikiart = list(map(lambda x: DATASET_PATH + 'wikiart_reduced/' + x,wikiart))
print('WikiArt Paths Loaded')
with open(METADATA_PATH + 'rijk_train.txt','r') as f:
  rijk = [DATASET_PATH + 'smaller/'+line.strip() for line in f.readlines()]
print('Rijkmusuem Paths Loaded')
imet = pd.read_csv(METADATA_PATH + 'imet_train_split.csv')
imet = imet.id.to_list()
imet = list(map(lambda x: f'{DATASET_PATH}imet_reduced/train/{x}.jpg',imet))
print('I-Met Paths Loaded')

# Concat all image paths
img_files = wikiart + rijk + imet
# Shuffle paths
img_files = np.array(img_files)
np.random.shuffle(img_files)

BATCH_SIZE = 128
STEPS = len(img_files) // BATCH_SIZE 

ds = tf.data.Dataset.from_tensor_slices(img_files) # Create dataset
ds = ds.shuffle(len(img_files)) # Shuffle
ds = ds.repeat() # Repeat forever
ds = ds.map(load_and_upscale,tf.data.experimental.AUTOTUNE) # load and upscale image if needed
ds = ds.prefetch(tf.data.experimental.AUTOTUNE) # prefetch
ds = ds.map(gen_patches,tf.data.experimental.AUTOTUNE) # generate two patchs
ds = ds.map(data_aug_pipeline_two,tf.data.experimental.AUTOTUNE) # augments patches
ds = ds.batch(BATCH_SIZE) # batch
ds = ds.prefetch(tf.data.experimental.AUTOTUNE)

if os.path.isdir(MODEL_PATH + 'query_encoder'):
  print('Loading Model From Disk')
  query_encoder,key_encoder,queue,optim = load_state(MODEL_PATH)
else:
  print('Creating New Model')
  cnn = keras.applications.Xception(
      include_top=False,weights=None,
      input_shape = (128,128,3),
      pooling='avg'
  )

  regularizer = tf.keras.regularizers.l2(1e-4)

  for layer in cnn.layers:
      for attr in ['kernel_regularizer']:
          if hasattr(layer, attr):
            setattr(layer, attr, regularizer)
  cnn_json = cnn.to_json()
  cnn = tf.keras.models.model_from_json(cnn_json)

  regularizer = keras.regularizers.L2(1e-4)
  mlp = keras.Sequential(
      [
      L.Dense(2048,'relu',kernel_regularizer=regularizer),
      L.Dense(128,'linear')
      ]
  )
  query_encoder = keras.Sequential([
    cnn, mlp
  ])
  key_encoder = keras.models.clone_model(query_encoder)
  key_encoder.set_weights(query_encoder.get_weights())

  queue = tf.random.normal([2**16,128])
  optim = keras.optimizers.Adam(1e-3)

SAVE_STEPS = 2000
TOTAL_STEPS = 200000
CHECK_POINTS = [30,60,100]
N_LOOPS = TOTAL_STEPS // SAVE_STEPS


for i in range(N_LOOPS):
  # Train model for number of steps then save
  query_encoder,key_encoder,queue = train_epoch(ds,optim,query_encoder,key_encoder,queue,SAVE_STEPS)
  checkpoint = str((i+1)*2) + 'k' if i + 1 in CHECK_POINTS else ''
  save_state(query_encoder,key_encoder,queue,optim,MODEL_PATH,step=checkpoint)
