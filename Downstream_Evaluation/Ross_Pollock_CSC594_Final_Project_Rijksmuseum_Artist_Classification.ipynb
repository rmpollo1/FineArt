{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ross_Pollock_CSC594_Final_Project_Rijksmuseum_Artist_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxs-FCk-eRZC"
      },
      "source": [
        "# Final Project: Rijksmuseum Artist Classification Task\n",
        "## CSC 594: Advanced Deep Learning\n",
        "## Ross Pollock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOmvY-wIHPag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46955b8e-e273-4fe5-9f7d-82840d3720a6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras \n",
        "import tensorflow.keras.layers as L\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC_M1moKHq11"
      },
      "source": [
        "# Extract Data to local colab drive\n",
        "! tar xfz /drive/My\\ Drive/CSC594/Data/Image/Rijksmuseum.tar.gz -C /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q21Ijvt-KmS"
      },
      "source": [
        "\n",
        "\n",
        "'''\n",
        "Implementation of ResNet 50,101,152\n",
        "with optional width parameter for \n",
        "making wider ResNets\n",
        "'''\n",
        "\n",
        "class ResNetBlock(L.Layer):\n",
        "  def __init__(\n",
        "      self,n_filters,regularizer=keras.regularizers.L2(1e-3),\n",
        "      activation=L.ReLU,start=False,downsample=False\n",
        "    ):\n",
        "    super(ResNetBlock,self).__init__()\n",
        "    self.regularizer = regularizer\n",
        "    self.activation = activation\n",
        "    self.n_filters = n_filters\n",
        "    self.start = start\n",
        "    self.stride = 2 if downsample else 1\n",
        "\n",
        "\n",
        "    self.left_path = keras.Sequential(\n",
        "        [\n",
        "          L.Conv2D(self.n_filters,1,self.stride,padding='same',kernel_regularizer=self.regularizer),\n",
        "          L.BatchNormalization(),\n",
        "          self.activation(),\n",
        "          L.Conv2D(self.n_filters,3,1,padding='same',kernel_regularizer=self.regularizer),\n",
        "          L.BatchNormalization(),\n",
        "          self.activation(),\n",
        "          L.Conv2D(self.n_filters*4,1,1,padding='same',kernel_regularizer=self.regularizer),\n",
        "          L.BatchNormalization()\n",
        "        ]\n",
        "    )\n",
        "  \n",
        "    # If first block add convolution to residual path\n",
        "    # so that the number of output channels match\n",
        "    if self.start:\n",
        "      self.right_path = keras.Sequential(\n",
        "          [\n",
        "           L.Conv2D(self.n_filters*4,1,self.stride,'same',kernel_regularizer=self.regularizer),\n",
        "           L.BatchNormalization()\n",
        "          ]\n",
        "      )\n",
        "\n",
        "    \n",
        "\n",
        "  def call(self,input_tensor,training=False):\n",
        "    x = self.left_path(input_tensor)\n",
        "    if self.start:\n",
        "      y = self.right_path(input_tensor)\n",
        "    else:\n",
        "      y = input_tensor\n",
        "\n",
        "    z = tf.add(x,y)\n",
        "    return self.activation()(z)\n",
        "      \n",
        "\n",
        "class ResNetStack(L.Layer):\n",
        "  def __init__(self,\n",
        "      n_blocks,n_filters,\n",
        "      first_block=False,**kwargs\n",
        "    ):\n",
        "    super(ResNetStack,self).__init__()\n",
        "    blocks = [ \n",
        "      ResNetBlock(n_filters,start=(not i),downsample=(not i and not first_block),**kwargs) \n",
        "      for i in range(n_blocks)\n",
        "    ]\n",
        "    self.stack = keras.Sequential(\n",
        "        blocks\n",
        "    )\n",
        "    self.out_dim = n_filters * 4\n",
        "\n",
        "  def call(self,input_tensor,training=False):\n",
        "    return self.stack(input_tensor) \n",
        "\n",
        "def ResNet(n_layers,width=1,input_shape=None,**kwargs):\n",
        "    if n_layers not in [50,101,152]:\n",
        "      raise ValueError\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "        L.Input(input_shape),\n",
        "        L.Conv2D(64,7,2,padding='same'),\n",
        "        L.BatchNormalization(),\n",
        "        L.ReLU(),\n",
        "        L.MaxPool2D(3,2,'same')\n",
        "        \n",
        "    ],name=f'ResNet{n_layers}')\n",
        "\n",
        "\n",
        "    model_specs = {\n",
        "        50: [3,4,6,3],\n",
        "        101: [3,4,23,3],\n",
        "        152: [3,8,36,3]\n",
        "    }\n",
        "    filters = list(map(lambda x: x*width,[64,128,256,512]))\n",
        "\n",
        "    for i,(stack,f) in enumerate(zip(model_specs[n_layers],filters)):\n",
        "      if i == 0:\n",
        "        stack = ResNetStack(stack,f,first_block=True)\n",
        "      else:\n",
        "        stack = ResNetStack(stack,f,first_block=False)\n",
        "      model.add(stack)\n",
        "    model.add(L.GlobalAveragePooling2D())\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkdURcCpgv4G"
      },
      "source": [
        "## Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPeBm5AYIprs",
        "outputId": "aa73efb4-6553-4c43-c298-fbdb6bbb2c8c"
      },
      "source": [
        "model_url = 'https://rosspollock.design/models/resnet50x2_120k.tar.gz'\n",
        "keras.utils.get_file('model',model_url,untar=True,extract=True)\n",
        "encoder = keras.models.load_model('/root/.keras/datasets/query_encoder120k')\n",
        "encoder = encoder.get_layer('ResNet50')\n",
        "if encoder.name == 'ResNet50':\n",
        "  new_encoder = ResNet(50,2,input_shape=(244,244,3))\n",
        "  new_encoder.set_weights(encoder.get_weights())\n",
        "  encoder = new_encoder \n",
        "  del new_encoder\n",
        "_, rep_dim = encoder.output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://rosspollock.design/models/resnet50x2_120k.tar.gz\n",
            "375029760/375028329 [==============================] - 7s 0us/step\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxr9Y_rPL6EJ"
      },
      "source": [
        "@tf.function\n",
        "def smart_resize(img,size=244):\n",
        "  h,w = tf.shape(img)[0], tf.shape(img)[1]\n",
        "  min_dim = tf.minimum(h,w)\n",
        "  ratio = size / tf.cast(min_dim,tf.float32) \n",
        "\n",
        "  new_w = tf.cast(w,tf.float32) * ratio\n",
        "  new_h = tf.cast(h,tf.float32) * ratio\n",
        "\n",
        "  img = tf.image.resize(img,[new_h,new_w],preserve_aspect_ratio=True)\n",
        "  img = tf.image.resize_with_crop_or_pad(img,size,size)\n",
        "  return img\n",
        "\n",
        "@tf.function\n",
        "def img_preprocess(fpath):\n",
        "    img = tf.io.read_file(fpath)\n",
        "    img = tf.image.decode_jpeg(img,3)\n",
        "    img = tf.cast(img,tf.float32)\n",
        "    img = smart_resize(img,size=244)\n",
        "    img /= (255/2)\n",
        "    img -= 1 \n",
        "    return img "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApOyavQ4gqxZ"
      },
      "source": [
        "## Load Image Paths and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rVAj6lmyeQ-"
      },
      "source": [
        "test_df = pd.read_csv('/drive/My Drive/CSC594/Data/Rijkmuseum_test_metadata.csv')\n",
        "filter_test = test_df[(test_df.object_creator != 'anonymous') & (test_df.object_creator != 'unknown')]\n",
        "filter_test = filter_test.groupby('object_creator').filter(lambda x: x['object_path'].count() > 10)\n",
        "\n",
        "train_df = pd.read_csv('/drive/My Drive/CSC594/Data/Rijkmuseum_train_metadata.csv')\n",
        "filter_train = train_df[train_df.object_creator.isin(filter_test.object_creator)]\n",
        "\n",
        "n_artist, _ = filter_test.groupby('object_creator').count().shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wjgZ4oK0faa"
      },
      "source": [
        "train_paths = filter_train.object_path.map(lambda x: '/content/smaller/'+x).to_list()\n",
        "test_paths = filter_test.object_path.map(lambda x: '/content/smaller/'+x).to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V1KsVmtK9dn"
      },
      "source": [
        "y_train = filter_train.object_creator\n",
        "y_test = filter_test.object_creator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drVERs8eLFDM"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--VbIIiqLXRl"
      },
      "source": [
        "n_train = y_train.shape[0]\n",
        "n_test = y_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF494cbegney"
      },
      "source": [
        "## Extract Representations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk51iOLpLtDr"
      },
      "source": [
        "x_train = np.zeros((n_train,rep_dim))\n",
        "x_test = np.zeros((n_test,rep_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2Xxmjy6L8Zb"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "ds_train = tf.data.Dataset.from_tensor_slices(train_paths)\n",
        "ds_train = ds_train.map(img_preprocess)\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "\n",
        "ds_test = tf.data.Dataset.from_tensor_slices(test_paths)\n",
        "ds_test = ds_test.map(img_preprocess)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoguPvlUM6ie",
        "outputId": "daeac2d0-c719-4d73-dd34-e749373724e7"
      },
      "source": [
        "pbar = keras.utils.Progbar(n_train // BATCH_SIZE + 1 )\n",
        "for i, batch in enumerate(ds_train):\n",
        "  reps = encoder(batch,training=False)\n",
        "  x_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:] = reps.numpy()\n",
        "  pbar.update(i+1)\n",
        "\n",
        "pbar = keras.utils.Progbar(n_test // BATCH_SIZE + 1 )\n",
        "for i, batch in enumerate(ds_test):\n",
        "  reps = encoder(batch,training=False)\n",
        "  x_test[i*BATCH_SIZE:(i+1)*BATCH_SIZE,:] = reps.numpy()\n",
        "  pbar.update(i+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 475s 1s/step\n",
            "97/97 [==============================] - 124s 1s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grgDsQRrgjp7"
      },
      "source": [
        "## Fit Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQQzL0x8AoX_"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK2VBh7eNplm"
      },
      "source": [
        "clf = keras.Sequential([\n",
        "  L.Dense(label_encoder.classes_.shape[0],'softmax')\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCoumXpOHJ1"
      },
      "source": [
        "clf.compile(\n",
        "    keras.optimizers.Adam(),\n",
        "    keras.losses.SparseCategoricalCrossentropy(),\n",
        "    keras.metrics.SparseCategoricalAccuracy()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNln5HHBOVqS",
        "outputId": "c8b4ee95-34df-43af-df41-8c0a69161329"
      },
      "source": [
        "clf.fit(x=x_train,y=y_train,epochs=50,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1198/1198 [==============================] - 4s 3ms/step - loss: 3.7891 - sparse_categorical_accuracy: 0.2470 - val_loss: 3.2016 - val_sparse_categorical_accuracy: 0.3331\n",
            "Epoch 2/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.9202 - sparse_categorical_accuracy: 0.3727 - val_loss: 2.9049 - val_sparse_categorical_accuracy: 0.3895\n",
            "Epoch 3/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.6408 - sparse_categorical_accuracy: 0.4192 - val_loss: 2.7732 - val_sparse_categorical_accuracy: 0.4126\n",
            "Epoch 4/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.4756 - sparse_categorical_accuracy: 0.4481 - val_loss: 2.6835 - val_sparse_categorical_accuracy: 0.4312\n",
            "Epoch 5/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.3600 - sparse_categorical_accuracy: 0.4710 - val_loss: 2.6352 - val_sparse_categorical_accuracy: 0.4359\n",
            "Epoch 6/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.2715 - sparse_categorical_accuracy: 0.4849 - val_loss: 2.5914 - val_sparse_categorical_accuracy: 0.4481\n",
            "Epoch 7/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.2014 - sparse_categorical_accuracy: 0.4981 - val_loss: 2.5712 - val_sparse_categorical_accuracy: 0.4511\n",
            "Epoch 8/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.1431 - sparse_categorical_accuracy: 0.5098 - val_loss: 2.5423 - val_sparse_categorical_accuracy: 0.4536\n",
            "Epoch 9/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.0932 - sparse_categorical_accuracy: 0.5163 - val_loss: 2.5351 - val_sparse_categorical_accuracy: 0.4587\n",
            "Epoch 10/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.0515 - sparse_categorical_accuracy: 0.5256 - val_loss: 2.5163 - val_sparse_categorical_accuracy: 0.4581\n",
            "Epoch 11/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 2.0144 - sparse_categorical_accuracy: 0.5332 - val_loss: 2.5023 - val_sparse_categorical_accuracy: 0.4631\n",
            "Epoch 12/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.9819 - sparse_categorical_accuracy: 0.5395 - val_loss: 2.5001 - val_sparse_categorical_accuracy: 0.4660\n",
            "Epoch 13/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.9515 - sparse_categorical_accuracy: 0.5463 - val_loss: 2.4954 - val_sparse_categorical_accuracy: 0.4625\n",
            "Epoch 14/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.9259 - sparse_categorical_accuracy: 0.5508 - val_loss: 2.4828 - val_sparse_categorical_accuracy: 0.4682\n",
            "Epoch 15/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.9001 - sparse_categorical_accuracy: 0.5535 - val_loss: 2.4827 - val_sparse_categorical_accuracy: 0.4680\n",
            "Epoch 16/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.8763 - sparse_categorical_accuracy: 0.5567 - val_loss: 2.4815 - val_sparse_categorical_accuracy: 0.4664\n",
            "Epoch 17/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.8577 - sparse_categorical_accuracy: 0.5615 - val_loss: 2.4870 - val_sparse_categorical_accuracy: 0.4735\n",
            "Epoch 18/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.8367 - sparse_categorical_accuracy: 0.5670 - val_loss: 2.4755 - val_sparse_categorical_accuracy: 0.4695\n",
            "Epoch 19/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.8202 - sparse_categorical_accuracy: 0.5695 - val_loss: 2.4748 - val_sparse_categorical_accuracy: 0.4698\n",
            "Epoch 20/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.8047 - sparse_categorical_accuracy: 0.5716 - val_loss: 2.4773 - val_sparse_categorical_accuracy: 0.4716\n",
            "Epoch 21/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.7867 - sparse_categorical_accuracy: 0.5775 - val_loss: 2.4812 - val_sparse_categorical_accuracy: 0.4726\n",
            "Epoch 22/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.7727 - sparse_categorical_accuracy: 0.5776 - val_loss: 2.4890 - val_sparse_categorical_accuracy: 0.4686\n",
            "Epoch 23/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.7595 - sparse_categorical_accuracy: 0.5808 - val_loss: 2.4847 - val_sparse_categorical_accuracy: 0.4744\n",
            "Epoch 24/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.7461 - sparse_categorical_accuracy: 0.5835 - val_loss: 2.4880 - val_sparse_categorical_accuracy: 0.4742\n",
            "Epoch 25/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.7336 - sparse_categorical_accuracy: 0.5856 - val_loss: 2.4855 - val_sparse_categorical_accuracy: 0.4770\n",
            "Epoch 26/50\n",
            "1198/1198 [==============================] - 4s 3ms/step - loss: 1.7221 - sparse_categorical_accuracy: 0.5892 - val_loss: 2.4808 - val_sparse_categorical_accuracy: 0.4768\n",
            "Epoch 27/50\n",
            "1198/1198 [==============================] - 4s 3ms/step - loss: 1.7102 - sparse_categorical_accuracy: 0.5915 - val_loss: 2.4822 - val_sparse_categorical_accuracy: 0.4770\n",
            "Epoch 28/50\n",
            "1198/1198 [==============================] - 4s 3ms/step - loss: 1.6996 - sparse_categorical_accuracy: 0.5922 - val_loss: 2.4863 - val_sparse_categorical_accuracy: 0.4753\n",
            "Epoch 29/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6905 - sparse_categorical_accuracy: 0.5945 - val_loss: 2.4897 - val_sparse_categorical_accuracy: 0.4762\n",
            "Epoch 30/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6797 - sparse_categorical_accuracy: 0.5974 - val_loss: 2.4962 - val_sparse_categorical_accuracy: 0.4780\n",
            "Epoch 31/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6705 - sparse_categorical_accuracy: 0.5982 - val_loss: 2.4970 - val_sparse_categorical_accuracy: 0.4764\n",
            "Epoch 32/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6622 - sparse_categorical_accuracy: 0.5996 - val_loss: 2.5026 - val_sparse_categorical_accuracy: 0.4795\n",
            "Epoch 33/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6533 - sparse_categorical_accuracy: 0.6036 - val_loss: 2.5016 - val_sparse_categorical_accuracy: 0.4805\n",
            "Epoch 34/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6448 - sparse_categorical_accuracy: 0.6020 - val_loss: 2.4995 - val_sparse_categorical_accuracy: 0.4798\n",
            "Epoch 35/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6360 - sparse_categorical_accuracy: 0.6051 - val_loss: 2.5091 - val_sparse_categorical_accuracy: 0.4764\n",
            "Epoch 36/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6286 - sparse_categorical_accuracy: 0.6070 - val_loss: 2.5065 - val_sparse_categorical_accuracy: 0.4775\n",
            "Epoch 37/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6207 - sparse_categorical_accuracy: 0.6084 - val_loss: 2.5178 - val_sparse_categorical_accuracy: 0.4766\n",
            "Epoch 38/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6138 - sparse_categorical_accuracy: 0.6098 - val_loss: 2.5181 - val_sparse_categorical_accuracy: 0.4777\n",
            "Epoch 39/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.6071 - sparse_categorical_accuracy: 0.6110 - val_loss: 2.5225 - val_sparse_categorical_accuracy: 0.4762\n",
            "Epoch 40/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5990 - sparse_categorical_accuracy: 0.6148 - val_loss: 2.5241 - val_sparse_categorical_accuracy: 0.4752\n",
            "Epoch 41/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5944 - sparse_categorical_accuracy: 0.6139 - val_loss: 2.5203 - val_sparse_categorical_accuracy: 0.4779\n",
            "Epoch 42/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5869 - sparse_categorical_accuracy: 0.6154 - val_loss: 2.5257 - val_sparse_categorical_accuracy: 0.4831\n",
            "Epoch 43/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5805 - sparse_categorical_accuracy: 0.6172 - val_loss: 2.5286 - val_sparse_categorical_accuracy: 0.4755\n",
            "Epoch 44/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5755 - sparse_categorical_accuracy: 0.6183 - val_loss: 2.5324 - val_sparse_categorical_accuracy: 0.4787\n",
            "Epoch 45/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5673 - sparse_categorical_accuracy: 0.6197 - val_loss: 2.5365 - val_sparse_categorical_accuracy: 0.4774\n",
            "Epoch 46/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5649 - sparse_categorical_accuracy: 0.6200 - val_loss: 2.5347 - val_sparse_categorical_accuracy: 0.4798\n",
            "Epoch 47/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5574 - sparse_categorical_accuracy: 0.6210 - val_loss: 2.5403 - val_sparse_categorical_accuracy: 0.4815\n",
            "Epoch 48/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5519 - sparse_categorical_accuracy: 0.6243 - val_loss: 2.5422 - val_sparse_categorical_accuracy: 0.4815\n",
            "Epoch 49/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5483 - sparse_categorical_accuracy: 0.6225 - val_loss: 2.5468 - val_sparse_categorical_accuracy: 0.4792\n",
            "Epoch 50/50\n",
            "1198/1198 [==============================] - 3s 3ms/step - loss: 1.5420 - sparse_categorical_accuracy: 0.6247 - val_loss: 2.5472 - val_sparse_categorical_accuracy: 0.4777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4a07212ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sJ3YB_MvxwK",
        "outputId": "c96a5e3f-f5e3-4aad-93cd-b7eb8645853e"
      },
      "source": [
        "test_loss, test_acc = clf.evaluate(x_test,y_test)\n",
        "print(f'Test Accuracy {100*test_acc:0.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "386/386 [==============================] - 1s 2ms/step - loss: 2.6020 - sparse_categorical_accuracy: 0.4734\n",
            "Test Accuracy 47.34\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}